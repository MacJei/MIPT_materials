## Вычислительный кластер. Принципы организации и системы управления (запуск задач, контроль целостности кластера, файловая система, сети передачи данных). Проблемы создания вычислительных кластеров с огромным числом узлов.

Вычислительный центр состоит из множества вычислительных систем, обычно расположенных в одном здании или в соседних. Вычислительная система — это набор компьютеров со своей внутренней сетью. Все компьютеры в сети, и пользователи получают доступ к данным из интернета.

Интерфейс взаимодействия вычислительной системы и пользователя представляет собой веб-сайт, на котором есть возможность регистрации. Обычно веб-сайт плюс почтовый клиент. Они выполняют следующие функции:

* идентификация пользователя;
* информирование о проблемах;
* получение отчетов об использовании вычислительной системы за год работы.

Узлы в рамках кластера соединены посредством интерконнекта. Из чего именно состоит вычислительная система:

* контрольный сервер (может, быть несколько — в зависимости ри масштаба кластера), который следит за состоянием оборудования (узлов кластера) и сообщает о проблемах, возможно, выключает всю систему;
* интерфейсовая машина (не машина, а сущность, но их может быть много) — на нее заходит пользователь и там он компилирует и редактирует, но не запускает программы;
* узлы кластера, связанные сетью; на них все считается;
* сервер очередей — пусть приходит много задач в секунду, а процессоров всего 200000; сервер очередй осуществляет управление задачами;
* storage (по сути отдельный кластер), нужный для хранения посчитанных данных; обычно данные хранятся отдельно, а не на узлах.

Виды сетей в вычислительной системе:

* управляющая сеть. Обычно никто не использует для вычислений. Она нужна для мониторинга состояний узлов и для подгрузки кода, который выполняется на узлах. По сути это обычная компьютерная сеть;
* сеть синхронизации. Меняет тактовую частоту и таймеры процессоров, чтобы синхронизировать их точнее, чем NTP (мировое время с точностью до секунд). Она плавное ускоряет часы отстающих машин и замедляет часы спешаших машин. Реализуется как коаксиальный кабель, по котором бегут импульсы точного времени;
* сеть ввода-вывода. Она может быть отдельной, а может быть объединена с вычислительной сетью между узлами. Все узлы соединить со storage дорого, поэтому соединяют каждый пятый или каждый десятый, а в остальные данные передаются через другие узлы через прокси;
* сеть взаимодействий типа «точка-точка». Нужна, если один узел в кластере хочет оченб быстро передать много информацию конкретному другому узлу;
* сеть коллективных операций. Нужна, если надо что-то распространять от одного узла ко всем остальным почти так же, как в сети взаимодействий типа «точка-точка».

Рассмотрим программное обеспечение для работы кластера. Нужно выполнять разные задачи, связанные с функционированием кластера.

* Мониторинг живучести узлов и оборудования. Ganglia собирает информацию о событиях в отдельных узлах в одном месте, строит базу данных и графики. Nagios анализирует ее и информирует, если что-то не так. Работает для небольших кластеров (меньше 10000 узлов). Если кластер большой, то встает задача анализа данных — поиска аномалий по логам и агрегироваие данных.

* Запуск узлов. Существует несколько важных проблем, которые нужно уметь решать.

  * Если запукать одновременно, то потребуется слишком много электроэнергии, следовательно, выделится много тепла и может произойти скачок напряжения. Поэтому нужно сначала задействовать систему охлаждения, а потом включать, но постепенно. Для большого кластера переход «ВЫКЛ–ВКЛ» может происходить весьма долгим. Для включения используется передача сигналов по SMPI (Service Manager Performance Indicator) (рассылка сообщений) и IPMI (Intelligent Platform Management Interface) (непосредственно включение).

  * Включенные узлы должны получить IP-адрес (Internet Protocol) и имя, обычно по DHCP (Dynamic Host Configuration Protocol). Одновременное включение большого числа узлов приведет к большой нагрузке на сеть, и пакеты с IP-адресами могут быть утеряны, что приведет к отсутствию адресов у некоторых узлов. Поэтому DHCP надо модифицировав, построив, например, звездчатую структуру (тогда нужно учесть, что будет нагрузка на промежуточные машины).

* Нужно взять образ ОС. Узлы делятся на два вида: дисковые и бездисковые, которым нужно разослать базовый образ ОС по TFTP (Trivial File Transfer Protocol). Они поднимают NFS (Network File System), который имеет только один сервер, так что на нем не хранятся пользовательские данные (иначе это станет узким местом системы). Используются параллельные файловые системы, то есть распределенные по кластеру. Ее задачи: расположить данные как можно ближе к тем, кто их использует, обеспечить POSIX-инетрфейс для обращений и надеждность хранения. Используем Lustre для получения ФС или GPFS (General Parallel File System) в кластерах IBM.

* Авторизация пользователей. Простейший способ — на каждом кластере хранить список passwd, shadow, group, gshadow). Когда заводим пользователя, запускает утилиту rsync, которая синхронизирует файл passwd. Преимущество этого подхода в том, что большой нагрузки на сеть нет даже при частых обращениях к данным пользователя. Однако так мы вынудены тратить больше времени, чтобы расшарить файлы на все узлы кластера.

  Второй способ — использовать LDAP (Lightweight Directory Access Protocol) — иерархическую базу данных для хранения сведений о пользователях. (Например, Activer Directory в Windows — одна из реализаций LDAP) Преимущества: не надо все хранить в конкретном узле, меняем данные в одном месте. Из недостатков нужно отметить большую нагрузку на сеть, так что программы на кластере могут тормозить.

* Средства разработки. Это зависит от железа, но скорее всего, стоит только одна из реализаций MPI.

  Можно использовать кросс-компиляции: двойной набор компиляции и библиотек. Кросс-компилятор компилирует программу не обязательно для той архитектуры, на которой он находится, а линковщик использует чужие библиотеке. Исполняемый файл отправляется на другую машину для исполнения.
