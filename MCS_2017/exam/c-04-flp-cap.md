## Теорема Фишера, Линч и Патерсона (FLP) (без доказательства). Решение задачи о консенсусе в синхронной системе. CAP-теорема (с доказательством). Разница между CAP- и FLP-теоремами.

### Формулировка FLP-теоремы

**Теорема Фишера, Линч и Патерсона** _(FLP Impossibility)_
> В асинхронной системе, построенной на передаче сообщений (AMP), не существует алгоритма консенсуса, допускающего Crash хотя бы одного процесса.

В билете без доказательства.

_Замечания_.
* Допускаем отказ типа Crash, следовательно, для более широкого класса отказов теорема о невозможности также остается верной.
* Идея доказательства в том, что на самом деле ни один процесс не примет решение. Это более сильное утверждение, так что из него следует теорема.
* Если разрешать только отказы типа Fail-Stop, то консенсус можно решить при некотором числе упавших процессов.
* В синхронной системе консенсус строится. Идея в том, что за такт сообщение обязано прийти, так что если этого не произошло, то мы гарантированно детектируем отказ.

### Напоминание из курса Concurrency: что такое линеаризуемость

Пусть мы написали многопоточный объект и собираемся использовать их сразу несколько.

Исполнения конкурирующих операций над объектами будем описывать с помощью _историй_. _История_ – последовательность событий вида «вызов начался» и «вызов завершился».

Назовем историю _последовательной_, если вызовы в ней не пересекаются, то есть за каждым событием вида «вызов начался» следует событие «вызов завершился». В противном случае назовем историю _конкурентной_.

История _H_ задает частичный порядок _<_ на операциях: два вызова упорядочены, если завершение первого вызова в истории идет раньше, чем начало второго.

Историю _H_ назовем _линеаризуемой_, если она объясняется последовательной историей, которая соблюдает частичный порядок _<_ (то есть существует последовательная история, все вызовы в которой возвращают те же значения, что и в истории _H_). Такую последовательную историю назовем _линеаризацией_ истории _H_. Назовем объект «хорошим», если любая конкурентная история вызовов над ним является линеаризуемой.

Основное утверждение в этой теории: история _H_ линеаризуема тогда и только тогда, когда линеаризуема история каждого каждого объекта в этой истории.

Вызовы операций в разных потоках могут быть упорядочены самой программой. Пусть объект сумел понять, что вызовы не упорядочены программой и конкурируют друг с другом. Тогда их можно применять в произвольном порядке, не нарушая при этом линеаризуемость. Наблюдатель (программа) не может рассчитывать на конкретный порядок их применения.

Иногда бывает, что для вызовов можно указать так называемые _точки линеаризации_, то есть такие точки, что порядок исполнения точек линеаризации отражал бы фактический порядок применения соответствующих операций.

Пояснение и интуиция: на временной прямой можно нарисовать отрезки времени, когда выполнялся тот или иной вызов в линеаризуемой истории. Из линеаризуемости следует, что эта история эквивалентна некоторой последовательность. Так вот, иногда бывает, что на отрезках исходной истории можно выбрать по одной точке так, что порядок точек соответствует данной последовательной истории (линеаризации). Как бы вызов совершен не в течение некоторого отрезка, а мгновенно в выбранной точке.

### CAP-теорема

Рассмотрим распределенную систему, реализующую базу данных, и потребуем от нее 3 свойства:

* (**Consistency**) Под консистентностью будем понимать линеаризуемость. Распределенная система должна быть линеаризуемой.
* (**Availability**) Доступность означает, что любой запрос, полученный исправным процессом, должен быть обработан.
* (**Partition Tolerance**) Устойчивость к сетевому разделению подразумевает, что произвольное число сообщений может теряться. Или процессы могут разделиться на две группы, между котороыми нет связности.

**Теорема CAP**
> В асинхронной распределенной системе невозможно имплементировать базы данных, удовлетворяющую CAP-свойствам.

**Доказательство**
* Пусть, напротив, это возможно, то есть существует такая база данных.
* Так как она распределенная, то в ней есть хотя бы 2 процесса. Разобьем все процессы на два непустых непесекающихся множества _A_ и _B_.
* Пусть _v_0_ — начальное состояние (значение) базы данных.
* Рассмотрим выполнение _a_, в котором была единственная запись в группе _A_, и пусть мы записали _v_1_ (это произойдет из-за Availability).
* Рассмотрим выполнение _b_, в котором было единственное чтение в группе _B_. Ясно, что оно могло вернуть только _v_0_  (это произойдет из-за Availability).
* Потребуем потери сообщений между _A_ и _B_. Так как выполнено свойство Partition Tolernce, то работы базы данных при этом не должна быть нарушена. Но теперь _B_ не знает ничего о том, что в _A_ что-то случилось.
* Рассмотрим выполение _ab_. Для процессов из группы _B_ выполнение _ab_ неотличимо от выполнения _b_, то есть чтение по-прежнему возвращает _v_0_ (это произойдет из-за Availability), так как он не знает про действия в _A_.
* Но тогда выполнение _ab_ нарушает свойство Consistency, так как история нелинеаризуема: чтение _b_ не увидело результат записи _a_, то есть эти вызовы не упорядочены.
* Пришли к противоречию (нарушение одного из свойств)

Оказывается, если потребовать выполнение только двух свойств, то это возможно.

* _CP-система_. Храним реплики в каждом процессе. Пробуем прочитать большинство значений в других процессах. Если не получилось достучаться до большинства (есть же Partition Tolerance), то говорим, что операция не успешна (можно, так как нет Availability). Если получилось, то возвращаем самое частое значение. Для записи аналогично: пытаемся записать в большинство, если не получилось, то сообщаем о провале операции.

* _AP-система_. Держим локальную базу данных и полностью не общаемся с другими. Возвращаем каждый раз свое число. В итоге все доступно, все партицируется, но возвращаем каждый разные числа, и так как нет консистентности, то это нормально.

* _CA-система_. Выделим один узел, который все будет хранить, а все остальные к нему обращаются.

### Критика CAP-теоремы

Свойства, требуемые в теореме, слишком сильны. Линеаризуемость вообще самое крутое, что можно потребовать от многопоточной (многопроцессорной) структуры данных.

Оказывается, ослабив требования, можно решить задачу. Например, задача t-Connected Consistency. Это выходит за рамки билета.

### Разница между FLP и CAP

«Упавший» узел в FLP не должен достигать консенсуса, так как на него все забили, а в CAP требуется, чтобы отделенный процесс (по сути — «упавший») все равно должен каким-то образом поддерживать актуальное состояние всей системы.
